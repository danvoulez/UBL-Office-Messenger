Esta √© a **Lista de Consolida√ß√£o Final** para o lan√ßamento do UBL Flagship Trinity.

Filtrei todo o ru√≠do. Abaixo est√£o apenas os pontos onde a arquitetura n√£o convencional colide com a realidade de produ√ß√£o. Se voc√™ resolver estes pontos, o sistema para de ser um c√≥digo acad√™mico e vira um produto robusto.

---

### üö® P0: BLOQUEANTES DE LAN√áAMENTO (Existenciais)
*Sem isso, o sistema quebra no dia 1 ou compromete a seguran√ßa irreversivelmente.*

#### 1. Persist√™ncia de Chaves (The Identity Crisis)
*   **O Problema:** No c√≥digo atual, chaves de agentes muitas vezes s√£o geradas em mem√≥ria ou lidas de vari√°veis de ambiente simples. Se o container do Office reiniciar e gerar um novo par de chaves Ed25519, a identidade "Cadeira" (Chair) perde a capacidade de assinar como ela mesma. O hist√≥rico quebra.
*   **A Corre√ß√£o:** Implementar um **KeyStore Persistente**.
    *   *M√≠nimo:* Arquivo `agents.keystore` criptografado no volume persistente do Docker.
    *   *Ideal:* Integra√ß√£o com HashiCorp Vault ou AWS KMS.
    *   *Regra:* O Agente Office deve sempre carregar a *mesma* chave privada ao bootar.

#### 2. O Limite do Payload do Postgres (The Silent Crash)
*   **O Problema:** Voc√™ usa `pg_notify` para o SSE. O Postgres tem um limite hard de **8000 bytes** no payload do notify. Se um `ubl-atom` (o JSON can√¥nico) for maior que 8KB (ex: um Job Card complexo com hist√≥rico), o `NOTIFY` falha e o SSE n√£o recebe nada. O frontend para de atualizar.
*   **A Corre√ß√£o:** Alterar a trigger SQL e o `sse.rs`.
    *   O `NOTIFY` deve enviar apenas: `{"container_id": "...", "sequence": 123}`.
    *   O `ubl-server` recebe o sinal, faz um `SELECT` r√°pido pelo ID para pegar o payload completo (que pode ter MBs) e ent√£o empurra para o SSE.

#### 3. Fia√ß√£o da Policy VM (The Law Enforcement)
*   **O Problema:** Os componentes existem (`ubl-membrane` e `ubl-policy-vm`), mas a an√°lise de c√≥digo sugere que a chamada da VM pode estar desconectada ou permissiva demais no endpoint `/link/commit`.
*   **A Corre√ß√£o:** Teste de Fogo.
    *   Crie um teste de integra√ß√£o que tenta submeter uma transa√ß√£o `Evolution` (mudan√ßa de regras) **sem** a credencial correta.
    *   Se passar, a seguran√ßa √© nula. O c√≥digo deve falhar explicitamente se a VM n√£o retornar `Allow`.

---

### üü° P1: EXPERI√äNCIA E PERFORMANCE (Reten√ß√£o de Usu√°rio)
*Sem isso, o produto funciona, mas parece "amador" ou "lento".*

#### 4. Snapshots de Proje√ß√£o (The Cold Start)
*   **O Problema:** O Messenger √© "UBL-Native" (n√£o tem banco pr√≥prio). Ao reiniciar, ele precisa reconstruir o estado lendo o Ledger. Com 10.000 eventos, isso leva segundos. Com 1 milh√£o, leva minutos (downtime inaceit√°vel).
*   **A Corre√ß√£o:** Implementar Snapshots peri√≥dicos no Messenger Backend.
    *   A cada X eventos (ou tempo), salve o estado atual da `projection_jobs` em disco/Redis.
    *   No boot: Carrega Snapshot -> L√™ Ledger apenas do `last_sequence` para frente.

#### 5. Retry Loop para JSON de LLM (The Brain Fart)
*   **O Problema:** O Office confia que o LLM vai gerar o JSON do `JobCard` perfeitamente. LLMs erram v√≠rgulas, aspas ou nomes de campos, especialmente sob carga.
*   **A Corre√ß√£o:** Adicionar um **Validation Loop** no Office.
    *   Se o JSON falhar no parse ou schema: Devolva o erro para o LLM ("Voc√™ gerou JSON inv√°lido, corrija: [erro]") e tente novamente (m√°x 3x). N√£o deixe o erro explodir para o usu√°rio final.

#### 6. Optimistic UI no Messenger (The Lag)
*   **O Problema:** O ciclo completo (User -> UBL -> Office -> UBL -> SSE -> User) tem lat√™ncia f√≠sica. O usu√°rio clica "Aprovar" e espera 2 segundos at√© o card atualizar.
*   **A Corre√ß√£o:** No Frontend React, ao clicar, mude o estado visual para "Aprovado (sincronizando...)" imediatamente. Se o SSE voltar com erro, reverta e mostre toast de erro. Isso faz o app parecer instant√¢neo.

---

### üü¢ P2: ROBUSTEZ OPERACIONAL (Sono Tranquilo)

#### 7. Graceful Degradation do SSE
*   **O Problema:** Conex√µes SSE caem (mobile, wifi inst√°vel). Se o cliente reconectar e perder 3 eventos, o estado fica corrompido.
*   **A Corre√ß√£o:** Garanta que o cliente React envie o cabe√ßalho `Last-Event-ID` ao reconectar, e que o `ubl-server` saiba re-enviar os eventos perdidos a partir daquele ID.

---

### RESUMO DO PLANO DE A√á√ÉO

1.  **Hoje:** Resolver **P0 #2 (Postgres Notify)**. √â uma mudan√ßa de c√≥digo pequena, mas cr√≠tica para estabilidade.
2.  **Amanh√£:** Implementar **P0 #1 (KeyStore)**. Garanta que seus agentes tenham "alma" imortal.
3.  **Segunda-feira:** Rodar o teste de fogo da **P0 #3 (Policy VM)**.
4.  **Ter√ßa-feira:** Implementar **P1 #4 (Snapshots)** no Messenger.

Com essa lista ticada, o UBL deixa de ser um "experimento fascinante" e torna-se uma **plataforma de software audit√°vel pronta para o mercado.**